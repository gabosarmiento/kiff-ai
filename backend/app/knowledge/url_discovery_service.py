"""
Comprehensive URL Discovery Service
==================================

Uses AGNO WebsiteReader to discover complete documentation structures for APIs.
Provides accurate URL counts for realistic indexing expectations.
"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

from agno.document.reader.website_reader import WebsiteReader
from app.knowledge.api_gallery import APIDocumentation, APIGallery

logger = logging.getLogger(__name__)

@dataclass
class URLDiscoveryResult:
    """Result of comprehensive URL discovery"""
    api_name: str
    display_name: str
    total_urls: int
    documentation_urls: int
    url_list: List[str]
    discovery_method: str
    timestamp: str
    error: Optional[str] = None

class URLDiscoveryService:
    """Service for comprehensive API documentation URL discovery"""
    
    def __init__(self):
        self.cache_dir = Path(__file__).parent.parent / "data" / "url_discovery_cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Configure WebsiteReader for comprehensive discovery
        self.website_reader = WebsiteReader(
            max_depth=4,      # Deep crawl for complete structure
            max_links=150,    # Allow many links for comprehensive coverage
            timeout=20        # Reasonable timeout for large sites
        )
        
        logger.info("ðŸ” URL Discovery Service initialized with AGNO WebsiteReader")
    
    async def discover_api_urls(self, api: APIDocumentation) -> URLDiscoveryResult:
        """Discover comprehensive URLs for a single API"""
        logger.info(f"ðŸ” Starting comprehensive URL discovery for {api.display_name}")
        
        try:
            # Strategy 1: Use configured URLs if available (like AGNO comprehensive files)
            if api.documentation_urls:
                logger.info(f"ðŸ“ Using configured URLs for {api.display_name}")
                
                # For comprehensive files, estimate sections that will be chunked
                if any('full.txt' in url or 'complete' in url for url in api.documentation_urls):
                    estimated_chunks = await self._estimate_comprehensive_file_chunks(api.documentation_urls)
                    
                    return URLDiscoveryResult(
                        api_name=api.name,
                        display_name=api.display_name,
                        total_urls=len(api.documentation_urls),
                        documentation_urls=estimated_chunks,
                        url_list=api.documentation_urls,
                        discovery_method="comprehensive_file_analysis",
                        timestamp=datetime.now().isoformat()
                    )
                else:
                    return URLDiscoveryResult(
                        api_name=api.name,
                        display_name=api.display_name,
                        total_urls=len(api.documentation_urls),
                        documentation_urls=len(api.documentation_urls),
                        url_list=api.documentation_urls,
                        discovery_method="configured_urls",
                        timestamp=datetime.now().isoformat()
                    )
            
            # Strategy 2: Use AGNO WebsiteReader for comprehensive crawling
            else:
                logger.info(f"ðŸ•·ï¸ Using WebsiteReader for comprehensive crawl of {api.display_name}")
                
                # Determine starting URL for crawling
                start_urls = self._get_crawl_starting_urls(api)
                
                all_discovered_urls = []
                successful_crawls = 0
                
                for start_url in start_urls:
                    try:
                        logger.info(f"   ðŸ” Crawling from: {start_url}")
                        results = self.website_reader.crawl(start_url)
                        
                        if results:
                            urls = list(results.keys())
                            all_discovered_urls.extend(urls)
                            successful_crawls += 1
                            logger.info(f"   âœ… Found {len(urls)} URLs from {start_url}")
                        else:
                            logger.warning(f"   âš ï¸ No URLs found from {start_url}")
                            
                    except Exception as e:
                        logger.warning(f"   âŒ Failed to crawl {start_url}: {e}")
                
                # Remove duplicates and filter for documentation URLs
                unique_urls = list(set(all_discovered_urls))
                doc_urls = self._filter_documentation_urls(unique_urls)
                
                logger.info(f"ðŸ“Š Discovery summary for {api.display_name}:")
                logger.info(f"   â€¢ Successful crawls: {successful_crawls}/{len(start_urls)}")
                logger.info(f"   â€¢ Total unique URLs: {len(unique_urls)}")
                logger.info(f"   â€¢ Documentation URLs: {len(doc_urls)}")
                
                return URLDiscoveryResult(
                    api_name=api.name,
                    display_name=api.display_name,
                    total_urls=len(unique_urls),
                    documentation_urls=len(doc_urls),
                    url_list=doc_urls,
                    discovery_method="website_reader_crawl",
                    timestamp=datetime.now().isoformat()
                )
                
        except Exception as e:
            logger.error(f"âŒ Error discovering URLs for {api.display_name}: {e}")
            return URLDiscoveryResult(
                api_name=api.name,
                display_name=api.display_name,
                total_urls=0,
                documentation_urls=0,
                url_list=[],
                discovery_method="error",
                timestamp=datetime.now().isoformat(),
                error=str(e)
            )
    
    async def discover_all_api_urls(self) -> Dict[str, URLDiscoveryResult]:
        """Discover comprehensive URLs for all APIs in the gallery"""
        logger.info("ðŸš€ Starting comprehensive URL discovery for all APIs")
        
        gallery = APIGallery()
        all_apis = gallery.get_all_apis()
        results = {}
        
        for api_name, api in all_apis.items():
            try:
                result = await self.discover_api_urls(api)
                results[api_name] = result
                
                # Add small delay to be respectful to servers
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"âŒ Failed to discover URLs for {api_name}: {e}")
                results[api_name] = URLDiscoveryResult(
                    api_name=api_name,
                    display_name=api.display_name,
                    total_urls=0,
                    documentation_urls=0,
                    url_list=[],
                    discovery_method="error",
                    timestamp=datetime.now().isoformat(),
                    error=str(e)
                )
        
        # Cache the results
        await self._cache_discovery_results(results)
        
        # Print summary
        self._print_discovery_summary(results)
        
        return results
    
    def _get_crawl_starting_urls(self, api: APIDocumentation) -> List[str]:
        """Get potential starting URLs for crawling"""
        base_url = api.base_url.rstrip('/')
        
        # Common documentation entry points
        candidates = [
            f"{base_url}/docs",
            f"{base_url}/documentation",
            f"{base_url}/api",
            f"{base_url}/developers",
            f"{base_url}/reference",
            f"{base_url}/getting-started",
            f"{base_url}/guide"
        ]
        
        # For specific known patterns
        if 'leonardo.ai' in base_url:
            candidates.insert(0, "https://docs.leonardo.ai/docs/getting-started")
        elif 'stability.ai' in base_url:
            candidates.insert(0, "https://platform.stability.ai/docs")
        elif 'elevenlabs.io' in base_url:
            candidates.insert(0, "https://elevenlabs.io/docs")
        elif 'openai.com' in base_url:
            candidates.insert(0, "https://platform.openai.com/docs")
        elif 'stripe.com' in base_url:
            candidates.insert(0, "https://stripe.com/docs")
        
        return candidates
    
    def _filter_documentation_urls(self, urls: List[str]) -> List[str]:
        """Filter URLs to keep only documentation-related ones"""
        doc_urls = []
        
        for url in urls:
            if any(indicator in url.lower() for indicator in [
                '/docs/', '/documentation/', '/api/', '/reference/', 
                '/guide/', '/tutorial/', '/getting-started', '/quickstart',
                '/developers/', '/dev/', '/sdk/', '/client/', '/integration/',
                '/webhook', '/auth', '/endpoint', '/model', '/generate'
            ]):
                doc_urls.append(url)
        
        return doc_urls
    
    async def _estimate_comprehensive_file_chunks(self, urls: List[str]) -> int:
        """Estimate chunks for comprehensive documentation files"""
        total_estimated = 0
        
        for url in urls:
            if any(keyword in url.lower() for keyword in ['full.txt', 'complete', 'comprehensive']):
                # For comprehensive files like AGNO's llms-full.txt, estimate based on typical structure
                total_estimated += 50  # Conservative estimate for comprehensive files
            else:
                total_estimated += 1
        
        return max(total_estimated, len(urls))
    
    async def _cache_discovery_results(self, results: Dict[str, URLDiscoveryResult]):
        """Cache discovery results for future use"""
        cache_file = self.cache_dir / "comprehensive_url_discovery.json"
        
        try:
            cache_data = {
                "timestamp": datetime.now().isoformat(),
                "total_apis": len(results),
                "results": {
                    name: {
                        "api_name": result.api_name,
                        "display_name": result.display_name,
                        "total_urls": result.total_urls,
                        "documentation_urls": result.documentation_urls,
                        "discovery_method": result.discovery_method,
                        "timestamp": result.timestamp,
                        "error": result.error
                    }
                    for name, result in results.items()
                }
            }
            
            with open(cache_file, 'w') as f:
                json.dump(cache_data, f, indent=2)
            
            logger.info(f"ðŸ’¾ Cached discovery results to {cache_file}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to cache results: {e}")
    
    def _print_discovery_summary(self, results: Dict[str, URLDiscoveryResult]):
        """Print comprehensive discovery summary"""
        logger.info("ðŸ“‹ COMPREHENSIVE URL DISCOVERY SUMMARY")
        logger.info("=" * 70)
        
        total_docs = 0
        successful_apis = 0
        
        for result in results.values():
            status_emoji = "âœ…" if result.error is None else "âŒ"
            doc_count = result.documentation_urls
            method = result.discovery_method
            
            logger.info(f"{status_emoji} {result.display_name}: {doc_count} URLs ({method})")
            
            if result.error is None:
                successful_apis += 1
                total_docs += doc_count
        
        logger.info("=" * 70)
        logger.info(f"ðŸŽ¯ TOTAL Documentation URLs: {total_docs}")
        logger.info(f"ðŸ“Š Successful APIs: {successful_apis}/{len(results)}")
        logger.info(f"ðŸ“š Average URLs per API: {total_docs/successful_apis if successful_apis > 0 else 0:.1f}")
        logger.info("ðŸš€ These are the REAL URL counts for production indexing!")

# Global service instance
_url_discovery_service: Optional[URLDiscoveryService] = None

def get_url_discovery_service() -> URLDiscoveryService:
    """Get the global URL discovery service instance"""
    global _url_discovery_service
    if _url_discovery_service is None:
        _url_discovery_service = URLDiscoveryService()
    return _url_discovery_service
